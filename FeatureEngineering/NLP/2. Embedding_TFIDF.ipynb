{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix,save_npz\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tfidf file count\n",
    "file_count = max([int(x[6:8].replace(\".\",\"\")) for x in os.listdir('tfidf') if '.' in x])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19da457691314398ac7d879fdebe20f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count the freq of doc having specific words\n",
    "have_word_count_dict = defaultdict(int)\n",
    "tot_doc_cnt = 0\n",
    "\n",
    "for i in tqdm(range(file_count)):\n",
    "    single_file = np.load('tfidf/value_%d.npy'%i,allow_pickle=True)\n",
    "    for doc in single_file:\n",
    "        for wrd in doc:\n",
    "            have_word_count_dict[wrd] += 1\n",
    "    tot_doc_cnt += len(single_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of words:index from frequency dictionary\n",
    "wrd_idx_map = {wrd:np.int32(idx) for idx,wrd in enumerate(have_word_count_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dictionary for each word with log of (total document count/count of word)\n",
    "for wrd, wrd_cnt in have_word_count_dict.items():\n",
    "    have_word_count_dict[wrd] = np.log(tot_doc_cnt/wrd_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5bd7303d3b427dbf213c1200200826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indice_list = []\n",
    "value_list = []\n",
    "dptr_list = [0]\n",
    "\n",
    "for i in tqdm(range(file_count)):\n",
    "    # Load npy file\n",
    "    single_file = np.load('tfidf/value_%d.npy'%i,allow_pickle=True)\n",
    "    \n",
    "    for doc_idx, doc in enumerate(single_file):\n",
    "        # Get words list from doc\n",
    "        wrd_list = [wrd for wrd in doc.keys()]\n",
    "        # Get words count from doc\n",
    "        wrd_cnt_list = list(doc.values())\n",
    "        # Multiply element-wise normalize word counts by their log of (total document count/count of word)\n",
    "        wrd_cnt_list = np.multiply(\n",
    "            np.divide(wrd_cnt_list, \n",
    "                      np.sum(wrd_cnt_list)\n",
    "                     ),\n",
    "            [have_word_count_dict.get(wrd) for wrd in wrd_list]\n",
    "        )\n",
    "        \n",
    "        # Add to end of indices, value, and dptr lists\n",
    "        indice_list.extend([wrd_idx_map.get(wrd) for wrd in wrd_list])\n",
    "        value_list.extend(wrd_cnt_list)\n",
    "        dptr_list.append(dptr_list[-1]+len(wrd_cnt_list))\n",
    "    \n",
    "# Save result to npz file\n",
    "    if (i+1) % 10 == 0:\n",
    "        result = csr_matrix((value_list, indice_list, dptr_list))\n",
    "        scipy.sparse.save_npz('tfidf/sparse_tfidf_mat/tfidf_%d.npz'%(i // 10), result)\n",
    "        indice_list = []\n",
    "        value_list = []\n",
    "        dptr_list = [0]\n",
    "        \n",
    "result = csr_matrix((value_list, indice_list, dptr_list))\n",
    "scipy.sparse.save_npz('tfidf/sparse_tfidf_mat/tfidf_%d.npz'%(i // 10), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea4b5e751be41c3ab7bb4d3616c9452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save to npy file\n",
    "index_list = []\n",
    "\n",
    "for i in tqdm(range(file_count)):\n",
    "    single_file = np.load('tfidf/index_%d.npy'%i,allow_pickle=True)\n",
    "    index_list.extend(single_file)\n",
    "        \n",
    "    if (i+1) % 10 == 0:\n",
    "        np.save('tfidf/sparse_tfidf_mat/index_%d.npy'%(i // 10), index_list)\n",
    "        index_list = []\n",
    "        \n",
    "np.save('tfidf/sparse_tfidf_mat/index_%d.npy'%(i // 10), index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
